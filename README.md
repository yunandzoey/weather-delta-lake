# WeatherÂ â†’Â Deltaâ€¯LakeÂ Pipeline (DatabricksÂ CE)

> **A handsâ€‘on demo pipeline** that ingests hourly weather forecasts, lands them in DeltaÂ Lake bronze, rolls up daily silver aggregates with dataâ€‘quality rules, and schedules everything on DatabricksÂ FreeÂ Edition (Serverless JobsÂ Compute).

![Architecture diagram](assets/architecture.png)

---

## ðŸ“ŒÂ Project status

| Milestone                              | âœ… Status | Key Artefacts                                                                |
| -------------------------------------- | -------- | --------------------------------------------------------------------------- |
| **1 Â· Bronze ingest (hourly)**         | âœ… Done   | `src/01_ingest_weather_bronze.ipynb` Â· `conf/job_weather_bronze.json`       |
| **2 Â· Silver daily + DQ**              | âœ… Done   | `src/02_transform_weather_silver.ipynb` Â· `conf/job_weather_pipeline.json`  |
| **3 Â· CI / Alerts**                    | âœ… Done   | GitHub Actions, Slack webhook, DQ failure alerts via job + webhook          |
| **4 Â· Perf tuning (OPTIMIZE / VACUUM)**| âœ… Done   | `src/03_maintenance_gold_tables.ipynb`                                      |
| **5 Â· SQL Dashboard (KPIs)**           | âœ… Done   | Databricks SQL: 7-day avg temp trend, DQ stats, card KPIs                   |


## Tech stack (100Â % freeâ€‘tier)

| Layer           | Tooling                                                          |
|----------------|------------------------------------------------------------------|
| Orchestration   | Databricks **Serverless Jobs Compute** (Free Edition)            |
| Processing      | PySpark 3.5 Â· Python 3.11                                        |
| Storage         | Delta Lake (bronze/silver tables on `hive_metastore`)            |
| CI / CD         | GitHub Actions Â· Databricks CLI v0.258                           |
| Testing         | Pytest Â· Spark Connect (local)                                   |
| Monitoring      | Email + Slack alerts if DQ fails or completeness <90%            |
| Docs / Dev Env  | `.devcontainer/` with VS Code + Python                          |

## Repo layout

```
weather-delta-lake/
â”œâ”€ .devcontainer/              # VSÂ Code cloudâ€‘dev env
â”‚   â”œâ”€ devcontainer.json
â”‚   â””â”€ Dockerfile
â”œâ”€ assets/
â”‚   â””â”€ architecture.png        # pipeline diagram
â”œâ”€ conf/
â”‚   â”œâ”€ job_weather_bronze.json # singleâ€‘task ingest
â”‚   â””â”€ job_weather_pipeline.json  # bronze âžœ silver job
â”œâ”€ src/
â”‚   â”œâ”€ 01_ingest_weather_bronze.ipynb    # 168â€‘h forecast â†’ bronze Delta
â”‚   â””â”€ 02_transform_weather_silver.ipynb # daily rollâ€‘up + DQ â†’ silver Delta
â”‚   â””â”€ 03_maintenance_gold_tables.ipynb # OPTIMIZE / VACUUM / rollups
â”œâ”€ tests/
â”‚   â””â”€ test_silver_transform.ipynb  # unitâ€‘test ideas / fixtures
â”œâ”€ LICENSE
â”œâ”€ pyproject.toml
â””â”€ README.md
```

## Pipeline flow

```mermaid
graph TD
    A["Open-Meteo API 168-hour forecast"] -->|hourly JSON| B(Bronze Delta\nweather_bronze.hourly)
    B -->|dedup & agg| C(Silver Delta: weather_silver.daily)
    C -->|perf tuning + rollup| D["Gold tables (weekly/monthly)"]
    D --> E["Databricks SQL Dashboard"]
```

* **Bronze**Â â€” appendâ€‘only raw JSON; partition by `ingest_ts_date`.
* **Silver**Â â€” daily upsert (MERGE) keyed by `date + lat + lon`.

### Dataâ€‘quality rules

| Rule         | Check                                      | Outcome                  |
|--------------|---------------------------------------------|---------------------------|
| Coverage     | 24 rows/day (unless it's the current date) | Fail task + log issue     |
| Temperature  | `-60 â‰¤ avg_temp_c â‰¤ 60`                    | Flag row (set `dq_passed` = false) |
| Humidity     | `0 â‰¤ min_humidity_pct â‰¤ 100`               | Flag row (set `dq_passed` = false) |
| DQ Summary   | `dq_passed` combines all rules             | Raise error if false rows |
| Alerting     | Completeness < 90% over 7 days             | Slack alert + fail job    |

A `dq_passed` boolean rolls up all three rules; task raises an exception if any `false` rows exist.

## Dashboard KPIs
Developed using **Databricks SQL Dashboard**, the following KPIs are visualized:
![Dashbord Picture](assets/dashbord_databricks.png)
- **ðŸ“ˆ 7-Day Rolling Average Temperature**  
  Trend line showing smoothed daily average temperature for the last two months

- **ðŸŒ¡ï¸ Current Temp (Â°C)**  
  Latest hourly temperature reading

- **âœ… Daily Completeness (last 7d, %)**  
  Percentage of past 7 days with complete hourly data (24 readings/day)

- **ðŸ’¨ Max Wind Speed (7d, km/h)**  
  Highest recorded wind speed in the last 7 days

- **ðŸ’§ Current Humidity (%)**  
  Latest hourly humidity reading

> **Location:** Berlin (52.52, 13.405)  
> **Dashboard Refresh:** Automatically updated via scheduled Databricks Jobs


## Quickâ€‘start for reviewers

```bash
# 0. Prereqs: Python 3.11, Databricks CLI >= 0.258, GitHub auth setup

# 1. Clone & install
$ git clone https://github.com/<your-gh>/weather-delta-lake.git
$ cd weather-delta-lake
$ pip install -r requirements-dev.txt

# 2. Log into your Databricks CE workspace
$ databricks auth login --host https://dbc-<hash>.cloud.databricks.com --token

# 3. Deploy pipeline job
$ databricks jobs create --json @conf/job_weather_pipeline.json

# 4. Backfill historical data (optional)
# Manually run 01_ingest_weather_bronze.ipynb with start/end dates

# 5. Trigger the pipeline
$ databricks jobs run-now --job-id <ID>

```

*(FreeÂ Edition automatically spins up Serverless compute; no cluster setup needed.)*

## Smokeâ€‘test SQL (databricks SQLÂ Editor)

```sql
-- Bronze sanity
-- Check Bronze ingest
SELECT ingest_ts_date, COUNT(*) AS rows
FROM   weather_bronze.hourly
GROUP  BY ingest_ts_date ORDER BY ingest_ts_date DESC;

-- Validate Silver DQ
SELECT date, row_count, dq_passed
FROM   weather_silver.daily
ORDER  BY date DESC;

-- View alerts
SELECT * FROM weather_silver.alerts ORDER BY alert_ts DESC;
```

## License

[MIT](LICENSE) â€“ free to fork, remix, learn.
