# WeatherÂ â†’Â Deltaâ€¯LakeÂ Pipeline (DatabricksÂ CE)

> **A handsâ€‘on demo pipeline** that ingests hourly weather forecasts, lands them in DeltaÂ Lake bronze, rolls up daily silver aggregates with dataâ€‘quality rules, and schedules everything on DatabricksÂ FreeÂ Edition (Serverless JobsÂ Compute).

![Architecture diagram](assets/architecture.png)

---

## ðŸ“ŒÂ Project status

| Milestone                           | Delivered  | Key artefacts                                                              |
| ----------------------------------- | ---------- | -------------------------------------------------------------------------- |
| **1Â Â·Â Bronze ingest (hourly)**      | âœ…Â Live     | `src/01_ingest_weather_bronze.ipynb` Â· `conf/job_weather_bronze.json`      |
| **2Â Â·Â Silver dailyÂ + DQ**           | âœ…Â Live     | `src/02_transform_weather_silver.ipynb` Â· `conf/job_weather_pipeline.json` |
| **3Â Â·Â CI / Alerts**                 | ðŸ”„Â Next    | GitHubÂ Action, Slack webhook                                               |
| 4Â Â·Â PerfÂ tuning (OPTIMIZE / VACUUM) | ðŸ”„Â Planned | maintenance notebook                                                       |
| 5Â Â·Â SQL dashboard (KPIs)            | ðŸ”„Â Planned | DatabricksÂ SQL artefacts                                                   |

## Tech stack (100Â % freeâ€‘tier)

| Layer                | Tooling                                                 |
| -------------------- | ------------------------------------------------------- |
| Orchestration        | Databricks **Serverless Jobs Compute** (FreeÂ Edition)   |
| Processing           | PySparkÂ 3.5 â€¢ PythonÂ 3.11                               |
| Storage              | DeltaÂ Lake â€” Bronze / Silver tables in `hive_metastore` |
| CI / CD              | GitHub Actions Â· DatabricksÂ CLIÂ v0.258                  |
| Testing              | Pytest + SparkÂ Connect (local)                          |
| Docs / DevÂ container | VSÂ CodeÂ DevÂ Containers (`.devcontainer/`)               |

## Repo layout

```
weather-delta-lake/
â”œâ”€ .devcontainer/              # VSÂ Code cloudâ€‘dev env
â”‚   â”œâ”€ devcontainer.json
â”‚   â””â”€ Dockerfile
â”œâ”€ assets/
â”‚   â””â”€ architecture.png        # pipeline diagram
â”œâ”€ conf/
â”‚   â”œâ”€ job_weather_bronze.json # singleâ€‘task ingest
â”‚   â””â”€ job_weather_pipeline.json  # bronze âžœ silver job
â”œâ”€ src/
â”‚   â”œâ”€ 01_ingest_weather_bronze.ipynb    # 168â€‘h forecast â†’ bronze Delta
â”‚   â””â”€ 02_transform_weather_silver.ipynb # daily rollâ€‘up + DQ â†’ silver Delta
â”œâ”€ tests/
â”‚   â””â”€ test_silver_transform.ipynb  # unitâ€‘test ideas / fixtures
â”œâ”€ LICENSE
â”œâ”€ pyproject.toml
â””â”€ README.md
```

## Pipeline flow

```mermaid
graph TD
    A["Open-Meteo API 168-hour forecast"] -->|hourly JSON| B(Bronze Delta\nweather_bronze.hourly)
    B -->|dedup & agg| C(Silver Delta: weather_silver.daily)
    C --> D["Databricks SQL Dashboard (planned)"]
```

* **Bronze**Â â€” appendâ€‘only raw JSON; partition by `ingest_ts_date`.
* **Silver**Â â€” daily upsert (MERGE) keyed by `date + lat + lon`.

### Dataâ€‘quality rules

| Rule            | Check                           | Action           |
| --------------- | ------------------------------- | ---------------- |
| **Coverage**    | finished days must have 24 rows | fail task if not |
| **Temperature** | âˆ’60Â â‰¤Â avgâ€¯Â°CÂ â‰¤Â 60               | flag row         |
| **Humidity**    | 0Â â‰¤Â minâ€¯%Â â‰¤Â 100                 | flag row         |

A `dq_passed` boolean rolls up all three rules; task raises an exception if any `false` rows exist.

##Â Quickâ€‘start for reviewers

```bash
# 0Â Prereqs: git Â· PythonÂ 3.11 Â· DatabricksÂ CLIÂ >=Â 0.258

# 1Â Clone & install dev deps
$ git clone https://github.com/<your-gh>/weather-delta-lake.git
$ cd weather-delta-lake
$ pip install -r requirements-dev.txt

# 2Â Log in to your DatabricksÂ CE workspace
$ databricks auth login --host https://dbc-<hash>.cloud.databricks.com --token

# 3Â Deploy the twoâ€‘task hourly pipeline
$ databricks jobs create --json @conf/job_weather_pipeline.json

# 4Â Kick off first run & tail logs
$ databricks jobs run-now --job-id <id>
```

*(FreeÂ Edition automatically spins up Serverless compute; no cluster setup needed.)*

## Smokeâ€‘test SQL (databricks SQLÂ Editor)

```sql
-- Bronze sanity
SELECT ingest_ts_date, COUNT(*) AS rows
FROM   weather_bronze.hourly
GROUP  BY ingest_ts_date ORDER BY ingest_ts_date DESC;

-- Silver DQ
SELECT date, row_count, dq_passed
FROM   weather_silver.daily;
```


## License

[MIT](LICENSE) â€“ free to fork, remix, learn.
