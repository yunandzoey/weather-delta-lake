{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbb2662-a008-4971-87d6-f078e20d46a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ---------- (cell 1) Widgets & helpers\n",
    "dbutils.widgets.text(\"start_date\", \"\")\n",
    "dbutils.widgets.text(\"end_date\",   \"\")\n",
    "start = dbutils.widgets.get(\"start_date\") or None\n",
    "end   = dbutils.widgets.get(\"end_date\")   or None\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import date\n",
    "from delta.tables import DeltaTable\n",
    "import requests\n",
    "\n",
    "# COMMAND ---------- (cell 2) Read Bronze, de‑dup, optional date filter\n",
    "bronze_raw = spark.read.table(\"weather_bronze.hourly\")\n",
    "\n",
    "bronze = bronze_raw.dropDuplicates(\n",
    "    [\"timestamp_utc\", \"location_lat\", \"location_lon\"]\n",
    ")\n",
    "\n",
    "if start or end:\n",
    "    bronze = bronze.filter(\n",
    "        (F.col(\"timestamp_utc\") >= F.lit(start) if start else F.lit(\"1900-01-01\")) &\n",
    "        (F.col(\"timestamp_utc\") <  F.lit(end)   if end   else F.lit(\"2999-12-31\"))\n",
    "    )\n",
    "\n",
    "df_daily = (\n",
    "    bronze\n",
    "    .withColumn(\"date\", F.to_date(\"timestamp_utc\"))\n",
    "    .groupBy(\"date\", \"location_lat\", \"location_lon\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"row_count\"),\n",
    "        F.avg(\"temp_c\").alias(\"avg_temp_c\"),\n",
    "        F.max(\"wind_speed_kmh\").alias(\"max_wind_kmh\"),\n",
    "        F.min(\"humidity_pct\").alias(\"min_humidity_pct\"),\n",
    "    )\n",
    "    .withColumn(\"process_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# COMMAND ---------- (cell 3) Data‑quality expectations\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"expect_row_count_ok\",\n",
    "    F.when(\n",
    "        F.col(\"date\") < F.current_date(),\n",
    "        F.col(\"row_count\") == 24\n",
    "    ).otherwise(F.col(\"row_count\") <= 24)\n",
    ")\n",
    "\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"expect_temp_ok\", F.col(\"avg_temp_c\").between(-60, 60)\n",
    ")\n",
    "\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"expect_humidity_ok\", F.col(\"min_humidity_pct\").between(0, 100)\n",
    ")\n",
    "\n",
    "dq_cols = [c for c in df_daily.columns if c.startswith(\"expect_\")]\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"dq_passed\",\n",
    "    F.expr(\" AND \".join(c for c in dq_cols))\n",
    ")\n",
    "\n",
    "# COMMAND ---------- (cell 4) Upsert into Delta Silver\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS weather_silver\")\n",
    "target = \"weather_silver.daily\"\n",
    "\n",
    "if not spark.catalog.tableExists(target):\n",
    "    (df_daily.write\n",
    "        .format(\"delta\")\n",
    "        .partitionBy(\"date\")\n",
    "        .saveAsTable(target))\n",
    "else:\n",
    "    tgt = DeltaTable.forName(spark, target)\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(\n",
    "            source=df_daily.alias(\"s\"),\n",
    "            condition=\"\"\"\n",
    "                t.date          = s.date AND\n",
    "                t.location_lat  = s.location_lat AND\n",
    "                t.location_lon  = s.location_lon\n",
    "            \"\"\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "# COMMAND ---------- (cell 5) Log all DQ issues & fail if needed\n",
    "\n",
    "# --- Identify failing rows ---\n",
    "bad = df_daily.filter(~F.col(\"dq_passed\"))\n",
    "\n",
    "if bad.count() > 0:\n",
    "    print(f\"⚠️ Logging {bad.count()} DQ issue(s) into alerts table...\")\n",
    "\n",
    "    # Ensure alerts table exists\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS weather_silver\")\n",
    "\n",
    "    # Prepare alert rows (one per failing day)\n",
    "    alerts_to_log = (\n",
    "        bad.withColumn(\"alert_date\", F.current_date())\n",
    "           .withColumn(\"dq_completeness_pct\", F.lit(None))  # placeholder for row-level issues\n",
    "           .withColumn(\"reason\", F.concat_ws(\n",
    "               \"; \",\n",
    "               F.when(~F.col(\"expect_row_count_ok\"), F.lit(\"Row Count Mismatch\")),\n",
    "               F.when(~F.col(\"expect_temp_ok\"), F.lit(\"Temp Out of Range\")),\n",
    "               F.when(~F.col(\"expect_humidity_ok\"), F.lit(\"Humidity Out of Range\"))\n",
    "           ))\n",
    "           .select(\"alert_date\", \"dq_completeness_pct\", \"reason\", \"date\", \"row_count\")\n",
    "    )\n",
    "\n",
    "    (alerts_to_log.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"weather_silver.alerts\"))\n",
    "\n",
    "    # Still fail the job (so email alerts trigger)\n",
    "    raise ValueError(f\"Data-quality failed for {bad.count()} day(s) in this batch\")\n",
    "\n",
    "else:\n",
    "    print(\"✅ No DQ issues in current batch.\")\n",
    "\n",
    "# COMMAND ---------- (Final Completeness DQ Alert Block – 7-day %)\n",
    "dq_check = spark.sql(\n",
    "    \"\"\"SELECT \n",
    "  ROUND((SUM(CASE WHEN expect_row_count_ok = TRUE THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 1)\n",
    "  AS dq_completeness_pct\n",
    "FROM weather_silver.daily\n",
    "WHERE date >= date_add(current_date(), -7)\"\"\"\n",
    ").collect()[0][\"dq_completeness_pct\"]\n",
    "\n",
    "if dq_check < 90:\n",
    "    alert_msg = f\"⚠️ Data Quality Alert: Daily Completeness is {dq_check}% (<90%)\"\n",
    "\n",
    "    # Log this as a separate alert row\n",
    "    spark.createDataFrame(\n",
    "        [(str(date.today()), dq_check, \"Completeness <90%\", None, None)],\n",
    "        [\"alert_date\", \"dq_completeness_pct\", \"reason\", \"date\", \"row_count\"]\n",
    "    ).write.format(\"delta\").mode(\"append\").saveAsTable(\"weather_silver.alerts\")\n",
    "\n",
    "    # Fail the job\n",
    "    raise ValueError(alert_msg)\n",
    "else:\n",
    "    print(f\"✅ DQ completeness OK: {dq_check}%\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_transform_weather_silver",
   "widgets": {
    "end_date": {
     "currentValue": "",
     "nuid": "45392a62-a75e-4567-bfbe-ab05ac6f3453",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "end_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "start_date": {
     "currentValue": "",
     "nuid": "74d1c7be-604a-4168-a936-8530bc2c7937",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "start_date",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
