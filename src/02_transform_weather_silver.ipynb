{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbb2662-a008-4971-87d6-f078e20d46a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# COMMAND ---------- (cell 1) Widgets & helpers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdbutils\u001b[49m.widgets.text(\u001b[33m\"\u001b[39m\u001b[33mstart_date\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m dbutils.widgets.text(\u001b[33m\"\u001b[39m\u001b[33mend_date\u001b[39m\u001b[33m\"\u001b[39m,   \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m start = dbutils.widgets.get(\u001b[33m\"\u001b[39m\u001b[33mstart_date\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "# COMMAND ---------- (cell 1) Widgets & helpers\n",
    "dbutils.widgets.text(\"start_date\", \"\")\n",
    "dbutils.widgets.text(\"end_date\",   \"\")\n",
    "start = dbutils.widgets.get(\"start_date\") or None\n",
    "end   = dbutils.widgets.get(\"end_date\")   or None\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import date\n",
    "from delta.tables import DeltaTable\n",
    "import requests\n",
    "\n",
    "# COMMAND ---------- (cell 2) Read Bronze, de‑dup, optional date filter\n",
    "bronze_raw = spark.read.table(\"weather_bronze.hourly\")\n",
    "\n",
    "bronze = bronze_raw.dropDuplicates(\n",
    "    [\"timestamp_utc\", \"location_lat\", \"location_lon\"]\n",
    ")\n",
    "\n",
    "if start or end:\n",
    "    bronze = bronze.filter(\n",
    "        (F.col(\"timestamp_utc\") >= F.lit(start) if start else F.lit(\"1900-01-01\")) &\n",
    "        (F.col(\"timestamp_utc\") <  F.lit(end)   if end   else F.lit(\"2999-12-31\"))\n",
    "    )\n",
    "\n",
    "df_daily = (\n",
    "    bronze\n",
    "    .withColumn(\"date\", F.to_date(\"timestamp_utc\"))\n",
    "    .groupBy(\"date\", \"location_lat\", \"location_lon\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"row_count\"),\n",
    "        F.avg(\"temp_c\").alias(\"avg_temp_c\"),\n",
    "        F.max(\"wind_speed_kmh\").alias(\"max_wind_kmh\"),\n",
    "        F.min(\"humidity_pct\").alias(\"min_humidity_pct\"),\n",
    "    )\n",
    "    .withColumn(\"process_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# COMMAND ---------- (cell 3) Data‑quality expectations\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"expect_row_count_ok\",\n",
    "    F.when(\n",
    "        F.col(\"date\") < F.current_date(),\n",
    "        F.col(\"row_count\") == 24\n",
    "    ).otherwise(F.col(\"row_count\") <= 24)\n",
    ")\n",
    "\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"expect_temp_ok\", F.col(\"avg_temp_c\").between(-60, 60)\n",
    ")\n",
    "\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"expect_humidity_ok\", F.col(\"min_humidity_pct\").between(0, 100)\n",
    ")\n",
    "\n",
    "dq_cols = [c for c in df_daily.columns if c.startswith(\"expect_\")]\n",
    "df_daily = df_daily.withColumn(\n",
    "    \"dq_passed\",\n",
    "    F.expr(\" AND \".join(c for c in dq_cols))\n",
    ")\n",
    "\n",
    "# COMMAND ---------- (cell 4) Upsert into Delta Silver\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS weather_silver\")\n",
    "target = \"weather_silver.daily\"\n",
    "\n",
    "if not spark.catalog.tableExists(target):\n",
    "    (df_daily.write\n",
    "        .format(\"delta\")\n",
    "        .partitionBy(\"date\")\n",
    "        .saveAsTable(target))\n",
    "else:\n",
    "    tgt = DeltaTable.forName(spark, target)\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(\n",
    "            source=df_daily.alias(\"s\"),\n",
    "            condition=\"\"\"\n",
    "                t.date          = s.date AND\n",
    "                t.location_lat  = s.location_lat AND\n",
    "                t.location_lon  = s.location_lon\n",
    "            \"\"\"\n",
    "        )\n",
    "        .whenMatchedUpdateAll()\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "# COMMAND ---------- (cell 5) Fail job immediately if DQ errors in current batch\n",
    "bad = df_daily.filter(~F.col(\"dq_passed\"))\n",
    "\n",
    "if bad.count() > 0:\n",
    "    raise ValueError(f\"Data‑quality failed for {bad.count()} day(s) in this batch\")\n",
    "\n",
    "# COMMAND ---------- (Final DQ Alert Block – 7-day completeness + alert logging)\n",
    "dq_check = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  ROUND((SUM(CASE WHEN expect_row_count_ok = TRUE THEN 1 ELSE 0 END) * 100.0) / COUNT(*), 1)\n",
    "  AS dq_completeness_pct\n",
    "FROM weather_silver.daily\n",
    "WHERE date >= date_add(current_date(), -7)\n",
    "\"\"\").collect()[0][\"dq_completeness_pct\"]\n",
    "\n",
    "alert_msg = None\n",
    "if dq_check < 90:\n",
    "    alert_msg = f\"⚠️ Data Quality Alert: Daily Completeness is {dq_check}% (<90%)\"\n",
    "\n",
    "    # ---- Option A: Slack Webhook ----\n",
    "    slack_webhook = \"https://hooks.slack.com/services/XXXX/YYYY/ZZZZ\"\n",
    "    try:\n",
    "        requests.post(slack_webhook, json={\"text\": alert_msg})\n",
    "    except Exception as e:\n",
    "        print(f\"Slack alert failed: {e}\")\n",
    "\n",
    "    # ---- Option B: Log alerts into a Delta table ----\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS weather_silver\")\n",
    "    alert_df = spark.createDataFrame(\n",
    "        [(str(date.today()), dq_check, \"Completeness <90%\")],\n",
    "        [\"alert_date\", \"dq_completeness_pct\", \"reason\"]\n",
    "    )\n",
    "    (alert_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(\"weather_silver.alerts\"))\n",
    "\n",
    "    # ---- Option C: Fail job to trigger email notifications ----\n",
    "    raise ValueError(alert_msg)\n",
    "else:\n",
    "    print(f\"✅ DQ completeness OK: {dq_check}%\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_transform_weather_silver",
   "widgets": {
    "end_date": {
     "currentValue": "",
     "nuid": "45392a62-a75e-4567-bfbe-ab05ac6f3453",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "end_date",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "end_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "start_date": {
     "currentValue": "",
     "nuid": "74d1c7be-604a-4168-a936-8530bc2c7937",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "start_date",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "",
      "label": null,
      "name": "start_date",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
